{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ac0b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:13:13.969231Z",
     "iopub.status.busy": "2025-04-22T13:13:13.968927Z",
     "iopub.status.idle": "2025-04-22T13:13:46.288114Z",
     "shell.execute_reply": "2025-04-22T13:13:46.287184Z",
     "shell.execute_reply.started": "2025-04-22T13:13:13.969204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc929c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:14:01.640275Z",
     "iopub.status.busy": "2025-04-22T13:14:01.639380Z",
     "iopub.status.idle": "2025-04-22T13:14:15.317442Z",
     "shell.execute_reply": "2025-04-22T13:14:15.316362Z",
     "shell.execute_reply.started": "2025-04-22T13:14:01.640245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b79bd053504a65b232229b2250c31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5652ba951d374c378ef7edea1a0c3e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c32ddce80cd48848d0acf00cf1bd3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c3e44d536542fe83d4c15a84fa4fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb2a87b114647e19a02e0d2c46a655d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1005026be3e4da0987aeb6ad5f78076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f342565d774f12ba55b6259dc11e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcbd98bbc37458ab9630c817a9cff79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd114629a34407eae387c450fd795db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load CLIP model from Hugging Face\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23339a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T13:18:16.294448Z",
     "iopub.status.busy": "2025-04-22T13:18:16.294133Z",
     "iopub.status.idle": "2025-04-22T13:18:16.692731Z",
     "shell.execute_reply": "2025-04-22T13:18:16.691747Z",
     "shell.execute_reply.started": "2025-04-22T13:18:16.294424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" mailcap rules found for type \"image/png\"\n",
      "/usr/bin/xdg-open: 882: www-browser: not found\n",
      "/usr/bin/xdg-open: 882: links2: not found\n",
      "/usr/bin/xdg-open: 882: elinks: not found\n",
      "/usr/bin/xdg-open: 882: links: not found\n",
      "/usr/bin/xdg-open: 882: lynx: not found\n",
      "/usr/bin/xdg-open: 882: w3m: not found\n",
      "xdg-open: no method available for opening '/tmp/tmpijzkjzff.PNG'\n"
     ]
    }
   ],
   "source": [
    "# path within Kaggle for Train the model \n",
    "image_folder = \"/kaggle/input/solesensei_bdd100k/bdd100k/bdd100k/images/10k/train/\"\n",
    "sample_image = os.path.join(image_folder, \"/kaggle/input/solesensei_bdd100k/bdd100k/bdd100k/images/10k/train/0004a4c0-d4dff0ad.jpg\")\n",
    "image = Image.open(sample_image)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b69a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text labels for scene description\n",
    "labels = [\"busy city street\", \"traffic accident\", \"construction site\", \"empty road\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb862902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process inputs\n",
    "inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ccaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits_per_image\n",
    "probs = logits.softmax(dim=1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "for label, prob in zip(labels, probs[0]):\n",
    "    print(f\"{label}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
